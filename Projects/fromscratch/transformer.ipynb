{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.optim \n",
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "import wandb\n",
    "import einops\n",
    "# import torch # use pytorch for now.\n",
    "from typing import *\n",
    "from dataclasses import dataclass, is_dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[3, 4]\n"
     ]
    }
   ],
   "source": [
    "def ceil_div(num : int, denom : int) -> int:\n",
    "    \"\"\"Saw interesting code that uses 'divmod'\"\"\"\n",
    "    q, r = divmod(num, denom)\n",
    "    return q + bool(r)\n",
    "    \n",
    "def get_k_from_iter(iter : Iterator, k : int):\n",
    "    \"\"\"Get k elements from iterator 'iter' \"\"\"\n",
    "    out = []\n",
    "    for _ in range(k):\n",
    "        try:\n",
    "            out.append(next(iter))\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def test():\n",
    "    k = [1, 2, 3, 4, 5].__iter__()\n",
    "    print(get_k_from_iter(k.__iter__(), 2))\n",
    "    print(get_k_from_iter(k.__iter__(), 2))\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    project : str # project this config belongs to.\n",
    "    nepochs : int \n",
    "    nbatches : int # number of batches per SGD step. \n",
    "    ministep_size : int # size of ministep in each batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DataClass = NewType('DataClass', Any)\n",
    "\n",
    "class DataCollator:\n",
    "    def __call__(self, data : List[DataClass]) -> DataClass:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class DefaultDataCollator(DataCollator):\n",
    "    def __call__(self, data : List[DataClass]) -> DataClass:\n",
    "        assert len(data) > 0\n",
    "        fs = data[0].fields()\n",
    "        collated = {} # collated data\n",
    "\n",
    "        for f in fs:\n",
    "            collated[f.name] = torch.stack([getattr(d, f.name) for d in data])\n",
    "        return DataClass(**collated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dataset:\n",
    "    def mk_iterator(self) -> Iterator[DataClass]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class IterableDataset(Dataset):\n",
    "    def __init__(self, data : Iterable[DataClass]):\n",
    "        self.data = data\n",
    "\n",
    "    def mk_iterator(self) -> Iterator[DataClass]:\n",
    "        return iter(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# partially ordered\n",
    "@dataclass\n",
    "class ValidationMetric:\n",
    "    epoch_loss : float \n",
    "    def is_strictly_worse_than(self, other : ValidationMetric) -> Optional[bool]:\n",
    "        \"\"\"return None if incomparable, True if self is better, False if other is better\"\"\"\n",
    "        fs = self.asdict()\n",
    "        otherfs = other.asdict()\n",
    "        assert fs.keys() == otherfs.keys()\n",
    "        return all([getattr(self, f.name) >= getattr(other, f.name) for f in fs])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainingState:\n",
    "    model : torch.nn.Module \n",
    "    batch_loss : torch.tensor\n",
    "    epoch_loss : float \n",
    "    bix : int \n",
    "    eix : int \n",
    "    data_iter : Iterator[DataClass]\n",
    "    metrics : List[ValidationMetric]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Evaluator:\n",
    "    def __call__(self,  dataset: Dataset, state: TrainingState) -> ValidationMetric:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class EpochLossEvaluator(Evaluator):\n",
    "    def __call__(self, dataset: Dataset, state: TrainingState) -> ValidationMetric:\n",
    "        return ValidationMetric(epoch_loss=state.epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainingCallback:\n",
    "    def before_train(self, config : Config, state : TrainingState) -> TrainingCommand:\n",
    "        return TrainingCommand()\n",
    "    def before_batch(self, config : Config, state : TrainingState) -> TrainingCommand:\n",
    "        return TrainingCommand()\n",
    "    def before_mini_step(self, config : Config, state : TrainingState) -> TrainingCommand:\n",
    "        return TrainingCommand()\n",
    "    def after_mini_step(self, config : Config, state : TrainingState) -> TrainingCommand:\n",
    "        return TrainingCommand()\n",
    "    def after_batch(self) -> TrainingCommand:\n",
    "        return TrainingCommand()\n",
    "    def before_epoch(self, config : Config, state : TrainingState) -> TrainingCommand:\n",
    "        return TrainingCommand()\n",
    "    def after_epoch(self) -> TrainingCommand:\n",
    "        return TrainingCommand()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLoop:\n",
    "    _config : Config \n",
    "    _model : torch.nn.Module \n",
    "    _dataset : Dataset\n",
    "    _collator : DataCollator\n",
    "    _evaluator : Evaluator\n",
    "    _optimizer : torch.optim.Optimizer\n",
    "    _state : TrainingState\n",
    "    _wandb_run : wandb.Run\n",
    "    \"\"\"Users should freely modify callback to add new callbacks\"\"\"\n",
    "    callbacks : List[TrainingCallback]\n",
    "\n",
    "    def __init__(self, config : Config, model : torch.nn.Module, dataset : Dataset,\n",
    "                 collator: DataCollator, evaluator : Evaluator,\n",
    "                 optimizer: torch.optim.Optimizer):\n",
    "        self._config = config \n",
    "        self._state = TrainingState(model=model,\n",
    "                                    batch_loss=0.0,\n",
    "                                    epoch_loss=0.0,\n",
    "                                    data_iter=dataset.mk_iterator())\n",
    "        self._model = model \n",
    "        self._dataset = dataset\n",
    "        self._collator = collator\n",
    "        self._evaluator = evaluator\n",
    "        self._optimizer = optimizer\n",
    "        self._wandb_run = wandb.init(config=self._config.asdict(), \n",
    "                               project=self._config.project,\n",
    "                               save_code=True,\n",
    "                               magic=True,\n",
    "                               config_exclude_keys=[\"project\"])\n",
    "        self.callbacks = []\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        for eix in range(self.config.nepochs):\n",
    "            self._state.eix = eix\n",
    "            for cb in self.callbacks:\n",
    "                cb.before_epoch(self, self._state)\n",
    "            self._state.epoch_loss = 0\n",
    "            self.run_epoch(self)\n",
    "            for cb in self.callbacks:\n",
    "                cb.after_epoch(self, self._state)\n",
    "        wandb.finish()\n",
    "\n",
    "    def run_epoch(self):\n",
    "        for bix in range(self.state.nbatches):\n",
    "            self._state.bix = bix\n",
    "            for cb in self.callbacks:\n",
    "                cb.before_batch(self, self._state)\n",
    "            self.run_batch(self)\n",
    "            \n",
    "        cur_validation = self._evaluator(self._dataset, self._state)\n",
    "        wandb.log(cur_validation.asdict())\n",
    "        is_pareto_frontier = True\n",
    "        for past_validation in self._state.metrics:\n",
    "            # if someone else is truly better than us,\n",
    "            # then we should not be added\n",
    "            if cur_validation.is_strictly_worse_than(past_validation):\n",
    "                is_pareto_frontier = False\n",
    "\n",
    "\n",
    "        self._state.metrics.append(cur_validation)\n",
    "        if is_pareto_frontier:\n",
    "            torch.save({\n",
    "                \"eix\": self._state.eix,\n",
    "                \"model\": self._model.state_dict(),\n",
    "                \"optim\": self._optimizer.state_dict(),\n",
    "            }, \"model-{self._state.eix}.pt\")\n",
    "\n",
    "    def run_batch(self):\n",
    "        size_per_batch = ceil_div(len(self._dataset), self._config.nbatches)\n",
    "        nministeps = ceil_div(size_per_batch, self._config.ministep_size)\n",
    "        self._state.batch_loss = torch.tensor(0.0, dtype=float)\n",
    "\n",
    "        self._optimizer.zero_grad()\n",
    "        for cb in self.callbacks:\n",
    "            cb.before_batch(self, self._state)\n",
    "\n",
    "        for msix in range(nministeps):\n",
    "            self.run_ministep(self, msix)\n",
    "\n",
    "        wandb.log({\"batch_loss\": self._state.batch_loss})\n",
    "        \n",
    "        self._optimizer.step()\n",
    "\n",
    "        for cb in self.callbacks:\n",
    "            cb.after_batch(self, self._state)\n",
    "\n",
    "    def run_ministep(self, msix : int):\n",
    "        # collate data for ministep\n",
    "        data = self._collator(get_k_from_iter(self._state.data_iter, self._config.ministep_size))\n",
    "        ms_loss = self._model.forward(data)\n",
    "        self._state.batch_loss += ms_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LinearConfig:\n",
    "    din : int \n",
    "    dout : int \n",
    "    # Ax + b\n",
    "    def Asize(self):\n",
    "        return [din, dout]\n",
    "    def bsize(self):\n",
    "        return [dout]\n",
    "\n",
    "\n",
    "class Linear(torch.nn.Module):\n",
    "    config : LinearConfig\n",
    "    W : torch.tensor \n",
    "    b : torch.tensor\n",
    "    def __init__(self, config: LinearConfig):\n",
    "        self.config = config\n",
    "        self.W = torch.random(size=config.Asize)\n",
    "        self.b = torch.random(size=config.bsize)\n",
    "        \n",
    "    def forward(self, x:torch.tensor):\n",
    "        # note that batch dim is always last dimension.\n",
    "        return einops.einsum(x, self.W, self.b, \"b in, in out, out -> b out\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2] 1 2\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class MaskKind:\n",
    "    MASKS = [CAUSAL, NONE] = list(range(2))\n",
    "    kind : int \n",
    "    def __init__(self, kind : int ):\n",
    "        assert kind in MaskKind.MASKS\n",
    "        self.kind = kind \n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SingleHeadAttnConfig:\n",
    "    din : int\n",
    "    dembed : int \n",
    "    dout : int \n",
    "    \n",
    "    @property\n",
    "    def Qconfig(self) -> LinearConfig: \n",
    "        return LinearConfig(self.din, self.dembed)\n",
    "    @property\n",
    "    def Kconfig(self) -> LinearConfig:\n",
    "        return LinearConfig(self.din, self.dembed)\n",
    "    @property\n",
    "    def Vconfig(self) -> LinearConfig:\n",
    "        return LinearConfig(self.din, self.dout)\n",
    "\n",
    "class SingleHeadAttn(torch.nn.Module):\n",
    "    config : SingleHeadAttnConfig\n",
    "    Q : Linear \n",
    "    K : Linear \n",
    "    V : Linear \n",
    "    def __init__(self, config : SingleHeadAttnConfig):\n",
    "        self.config = config \n",
    "        self.Q = Linear(config.Qconfig)\n",
    "        self.K = Linear(config.Kconfig)\n",
    "        self.V = Linear(config.Vconfig)\n",
    "\n",
    "    # note that the context is shared for /every/ xs.\n",
    "    # xs : [NBATCH;NX;DIN]\n",
    "    # ctxs:[NBATCH;NCTX;DIN]\n",
    "    # out: [NBATCH;NX;DOUT]\n",
    "    def forward(self, xs : torch.tensor, ctxs : torch.tensor, mask : MaskKind=None):\n",
    "        # {e, o} < c < b (less 'feature' dimensions are outer).\n",
    "        xsembeds = einops.einsum(self.Q.W, self.Q.b, xs, \"i e, e, b x i -> b x e\") # [NBATCH;NX;DEMBED]\n",
    "        keys = einops.einsum(self.K.W, self.K.b, ctxs, \"i e, e, b c i -> b c e\") #[NBATCH;NCTX;DEMBED]\n",
    "        vs = einops.einsum(self.V.W, self.V.b, ctxs, \"i o, o, b c i -> b c o\") # [NBATCH;NCTX;DOUT]\n",
    "        # \"attn matrix\" (unnormalized)\n",
    "        keys_dot_xs = einops.einsum(keys, xsembeds, \"b c e, b x e -> b c x\") # [NBATCH;NCTX;NX]\n",
    "        if mask.kind == MaskKind.CAUSAL:\n",
    "            # perform the fill upto the top\n",
    "            keys_dot_xs = torch.masked_fill()\n",
    "            raise RuntimeError(\"don't know how to fill mask with causal pattern.\")\n",
    "\n",
    "            keys_dot_xs = torch.masked_fill(input=keys_dot_xs, mask=mask, value=-torch.inf)\n",
    "        # keys_dots_xs = [NBATCH;NCTX;NX]\n",
    "        keys = torch.softmax(keys_dot_xs, dim=1) # take softmax along NCTX\n",
    "        # for each word, compute the output representation.\n",
    "        out = einops.einsum(keys, vs, \"b c x, b c o -> b x o\") # [NBATCH;NX;DOUT]\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for mask: https://pytorch.org/text/stable/_modules/torchtext/nn/modules/multiheadattention.html\n",
    "@dataclass \n",
    "class MultiHeadAttnConfig:\n",
    "    din : int \n",
    "    dembedSingleHead : int\n",
    "    doutSingleHead : int \n",
    "    dout : int \n",
    "    nheads : int\n",
    "    \n",
    "    @property\n",
    "    def singleHeadAttnConfig(self) -> SingleHeadAttnConfig:\n",
    "        return SingleHeadAttnConfig(din=self.din, dembed=self.dembedSingleHead, dout=self.doutSingleHead)\n",
    "    @property\n",
    "    def finalLinearConfig(self) -> LinearConfig:\n",
    "        return LinearConfig(din=self.nheads*self.doutSingleHead, dout=self.dout)\n",
    "\n",
    "class MultiHeadAttn(torch.nn.Module):\n",
    "    heads : List[SingleHeadAttn]\n",
    "    Wout : Linear\n",
    "    \n",
    "    def __init__(self, config : MultiHeadAttnConfig):\n",
    "        self.config = config \n",
    "        for hix in range(config.nheads):\n",
    "            self.heads.append(SingleHeadAttn(config.singleHeadAttnConfig))\n",
    "        self.Wout = Linear(config.finalLinearConfig)\n",
    "    \n",
    "    # xs : [NBATCH;NX;DEMBED]\n",
    "    # zs: [NBATCH;NCTX;DEMBED;],\n",
    "    # out: [NBATCH;NX;DOUT]\n",
    "    def forward(self, xs : torch.tensor, zs : torch.tensor, mask : torch.tensor):\n",
    "        ys = [head(xs, zs, mask) for head in self.heads] # ys = [NBATCH;]\n",
    "        #ys : [NBATCH;(NHEADS*DOUT_SINGLE)]\n",
    "        ys = einops.rearrange(ys, \"b h o  -> b (h o)\") # concatenat to get xs = [(NHEADS*DOUT_SINGLE);NBATCH]\n",
    "        outs = self.Wout(ys) # unsure this does what I want it to do lol\n",
    "        return outs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SelfAttentionDecoderConfig:\n",
    "    max_input_length : int \n",
    "    nvocab: int #number of words in vocabulary to produce probability distribution over. \n",
    "    dembedSingleHead : int\n",
    "    doutSingleHead : int  \n",
    "    nheads : int \n",
    "    nlayers : int # number of times to perform self attention\n",
    "    din : int # input embedding dimension\n",
    "    dembedSingleHead : int \n",
    "    doutSingleHead : int \n",
    "    \n",
    "    @property\n",
    "    def wordEmbeddingShape(self):\n",
    "        return (self.din, self.nvocab)\n",
    "\n",
    "    @property \n",
    "    def multiHeadAttnConfig(self):\n",
    "        return MultiHeadAttnConfig(dim=self.dim,\n",
    "                                nheads=self.nheads,\n",
    "                                dembedSingleHead=self.dembedSingleHead,\n",
    "                                doutSingleHead=self.doutSingleHead,\n",
    "                                dout=self.nvocab)\n",
    "\n",
    "# decoder only language model (e.g. gpt)\n",
    "class SelfAttentionDecoder(torch.nn.Module):\n",
    "    attns : MultiHeadAttn\n",
    "    word_embeds : torch.tensor\n",
    "    def __init__(self, config : SelfAttentionDecoderConfig):\n",
    "        self.config = config\n",
    "        self.word_embeds = torch.random(shape=config.wordEmbeddingShape)\n",
    "        self.attns = [MultiHeadAttn(self.config.multiHeadAttnConfig)]\n",
    "    # return probabilities\n",
    "    # xs: one hot encoding of words.\n",
    "    # xs: [[word]] = [sentence]\n",
    "    # outs: [NVOCAB;NWORDS;NBATCH] \n",
    "    # LorA at each step!\n",
    "    # for us, NCONTEXT = NWORDS\n",
    "    def foward(self, sentences: torch.tensor):\n",
    "        # please don't disappoint me, einops.\n",
    "        NWORDSMAX = max([len(sent) for sent in sentences])\n",
    "        assert NWORDSMAX < self.config.max_input_length\n",
    "        \n",
    "        # how to efficiently create mask?\n",
    "        # mask = ...\n",
    "        mask = None \n",
    "        # FIXME: batch dimension is **FIRST DIMENSION**\n",
    "        # xs: [NBATCH;NWORDSMAX;DIN]\n",
    "        xs = einops.rearrange([[self.word_embeds[:, wix] for wix in sentence] for sentence in sentences] , \"b w e -> e w b\")\n",
    "        \n",
    "        for l in range(self.layers):\n",
    "            # TODO: separate layer norm for each word in sentence, wtf.\n",
    "            xs_tilde = torch.nn.functional.layer_norm(input=xs,\n",
    "                                                      normalized_shape=(self.config.din, NWORDSMAX))\n",
    "            xs = xs + self.attns(xs_tilde, xs_tilde, mask) # residual\n",
    "            \n",
    "            xs_tilde = torch.nn.functional.layer_norm(input=xs)            \n",
    "            xs = xs + torch.nn.functional.gelu(...)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
